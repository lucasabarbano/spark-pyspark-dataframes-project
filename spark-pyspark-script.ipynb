{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some quick practice with Spark DataFrame tools.\n",
    "\n",
    "We will answer some basic questions about some stock market data, in this case Microsoft Stock from the years 2010-2019.\n",
    "\n",
    "We will learn:\n",
    "\n",
    "- How to start a Spark Session\n",
    "- How to load data\n",
    "- How to access some information from data\n",
    "- How to extract some descriptive statistics\n",
    "- How to use SQL syntax and SQL functions in python/pyspark to answer our questions\n",
    "- How to work with some time functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a simple Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/lucasabarbano/spark-2.4.5-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('DataFrame-Project').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Microsoft Stock CSV File, have Spark infer the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('msft_stock.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the Schema look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the first 5 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Date=datetime.datetime(2010, 1, 4, 0, 0), Open=30.620001, High=31.1, Low=30.59, Close=30.950001, Adj Close=24.294369, Volume=38409100)\n",
      "\n",
      "\n",
      "Row(Date=datetime.datetime(2010, 1, 5, 0, 0), Open=30.85, High=31.1, Low=30.639999, Close=30.959999, Adj Close=24.302216, Volume=49749600)\n",
      "\n",
      "\n",
      "Row(Date=datetime.datetime(2010, 1, 6, 0, 0), Open=30.879999, High=31.08, Low=30.52, Close=30.77, Adj Close=24.15307, Volume=58182400)\n",
      "\n",
      "\n",
      "Row(Date=datetime.datetime(2010, 1, 7, 0, 0), Open=30.629999, High=30.700001, Low=30.190001, Close=30.450001, Adj Close=23.901886, Volume=50559700)\n",
      "\n",
      "\n",
      "Row(Date=datetime.datetime(2010, 1, 8, 0, 0), Open=30.280001, High=30.879999, Low=30.24, Close=30.66, Adj Close=24.066734, Volume=51197400)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in df.head(5):\n",
    "    print(row)\n",
    "    print('\\n')\n",
    "\n",
    "#df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive statistics about the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+--------+--------+---------+-----------+\n",
      "|summary|    Open|    High|     Low|   Close|Adj Close|     Volume|\n",
      "+-------+--------+--------+--------+--------+---------+-----------+\n",
      "|  count|2,516.00|2,516.00|2,516.00|2,516.00| 2,516.00|      2,516|\n",
      "|   mean|   56.31|   56.77|   55.82|   56.32|    52.34| 39,926,492|\n",
      "| stddev|   33.87|   34.11|   33.57|   33.86|    35.11| 22,991,730|\n",
      "|    min|   23.09|   23.32|   22.73|   23.01|    18.23|  7,425,600|\n",
      "|    max|  159.45|  159.55|  158.22|  158.96|   158.53|319,317,888|\n",
      "+-------+--------+--------+--------+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "df_describe = df.describe()  \n",
    "\n",
    "df_describe.select(df_describe[\"summary\"],\n",
    "                  format_number(df_describe[\"Open\"].cast('float'),2).alias('Open'),\n",
    "                  format_number(df_describe[\"High\"].cast('float'),2).alias('High'),\n",
    "                  format_number(df_describe[\"Low\"].cast('float'),2).alias('Low'),\n",
    "                  format_number(df_describe[\"Close\"].cast('float'),2).alias('Close'),\n",
    "                  format_number(df_describe[\"Adj Close\"].cast('float'),2).alias('Adj Close'),\n",
    "                  format_number(df_describe[\"Volume\"].cast('float'),0).alias('Volume')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new variable, ratio of the High Price versus volume of stock traded for a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            HV Ratio|\n",
      "+--------------------+\n",
      "|8.097039503659289E-7|\n",
      "|6.251306543168187E-7|\n",
      "|5.341821581784181E-7|\n",
      "|6.072029897329296E-7|\n",
      "|6.031556094645432E-7|\n",
      "|4.473875967752023E-7|\n",
      "|4.612203222170132...|\n",
      "|5.884678049109682E-7|\n",
      "|4.918699122700192E-7|\n",
      "|3.909241527056856...|\n",
      "|6.707360275851999E-7|\n",
      "| 5.64089025424115E-7|\n",
      "|4.203226989315429...|\n",
      "|2.960650892214665E-7|\n",
      "|4.680226594922128E-7|\n",
      "|4.479298438322987E-7|\n",
      "|4.663054441395163E-7|\n",
      "|2.541831377958484...|\n",
      "|1.543154957617393...|\n",
      "|3.314283187344279...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('HV Ratio', df['High']/df['Volume']).select('HV Ratio').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What day had the Peak High in Price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 12, 27, 0, 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.orderBy(df['High'].desc()).head(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the mean of the Close column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|56.32216619634341|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('microsoft')\n",
    "\n",
    "# using sql syntax\n",
    "spark.sql('''\n",
    "          SELECT AVG(Close) FROM microsoft  \n",
    "          ''').show()\n",
    "\n",
    "# using sql functions\n",
    "#from pyspark.sql.functions import mean\n",
    "#df.select(mean(\"Close\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the max and min of the Volume column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|min(Volume)|max(Volume)|\n",
      "+-----------+-----------+\n",
      "|    7425600|  319317900|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using sql syntax\n",
    "spark.sql('''\n",
    "          SELECT MIN(Volume),MAX(Volume) FROM microsoft\n",
    "            ''').show()\n",
    "\n",
    "# using sql functions\n",
    "#from pyspark.sql.functions import max,min\n",
    "#df.select(min(\"Volume\"),max(\"Volume\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many days was the Close lower than 80 dollars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|count(Close)|\n",
      "+------------+\n",
      "|        1969|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using sql syntax\n",
    "spark.sql('''\n",
    "          SELECT COUNT(Close) FROM microsoft\n",
    "          WHERE Close < 80\n",
    "          ''').show()\n",
    "\n",
    "# using filter\n",
    "#df.filter(\"Close < 80\").count()\n",
    "#df.filter(df['Close'] < 80).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of the time was the High greater than 100 dollars ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of the time wich the high price greater than 100 dollas was 15.70%\n"
     ]
    }
   ],
   "source": [
    "# using spark.sql\n",
    "df_high100 = spark.sql('''\n",
    "                       SELECT COUNT('Date') FROM microsoft\n",
    "                       WHERE High > 100\n",
    "                       ''').collect()\n",
    "\n",
    "df_totaldays = spark.sql('''\n",
    "                         SELECT COUNT('Date') FROM microsoft\n",
    "                         ''').collect()\n",
    "\n",
    "print('The percentage of the time wich the high price greater than 100 dollas was {:0.2f}%'.\n",
    "      format(df_high100[0][0] / df_totaldays[0][0]*100))\n",
    "\n",
    "# A way easier to answer this question\n",
    "#df.filter(df[\"High\"]>100).count()/df.count()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Pearson correlation between High and Volume?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  corr(High, Volume)|\n",
      "+--------------------+\n",
      "|-0.44237956137895296|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "df.select(corr(\"High\",\"Volume\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the max High per year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Year| max(High)|\n",
      "+----+----------+\n",
      "|2010|     31.58|\n",
      "|2011| 29.459999|\n",
      "|2012| 32.950001|\n",
      "|2013|     38.98|\n",
      "|2014| 50.049999|\n",
      "|2015| 56.849998|\n",
      "|2016| 64.099998|\n",
      "|2017|      87.5|\n",
      "|2018|    116.18|\n",
      "|2019|159.550003|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "df_year = df.withColumn('Year', year(df['Date']))\n",
    "\n",
    "df_year.groupBy('Year').agg({'High':'max'}).orderBy('Year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average Close for each Calendar Month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|Month|        avg(Close)|\n",
      "+-----+------------------+\n",
      "|    1| 50.95534645544554|\n",
      "|    2| 50.82932285416666|\n",
      "|    3|   51.890321206422|\n",
      "|    4| 53.75874399516908|\n",
      "|    5|55.348591511737084|\n",
      "|    6| 55.00586861032864|\n",
      "|    7| 57.55094798104266|\n",
      "|    8|  57.9727803542601|\n",
      "|    9|57.987290802955684|\n",
      "|   10| 60.25800005909088|\n",
      "|   11|  61.7215610878049|\n",
      "|   12| 61.99248818181819|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month\n",
    "df_month = df.withColumn('Month', month(df['Date']))\n",
    "\n",
    "df_month.groupBy('Month').agg({'Close':'avg'}).orderBy('Month').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
